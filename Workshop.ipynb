{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pfilo8/Conditional-object-generation-using-pre-trained-models-and-plug-in-networks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %cd Conditional-object-generation-using-pre-trained-models-and-plug-in-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib nflows sklearn torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional object generation using pre-trained models and plug-in networks\n",
    "Prowadzący: **Patryk Wielopolski**, Politechnika Wrocławska\n",
    "\n",
    "Abstrakt: Modele generatywne przyciągnęły uwagę wielu praktyków uczenia maszynowego w ostatnich latach, co zaowocowało modelami takimi jak StyleGAN do generowania ludzkiej twarzy lub PointFlow do generowania chmur punktów 3D. Jednak domyślnie nie możemy kontrolować jego procesu próbkowania, tj. nie możemy wygenerować próbki z określonym zestawem atrybutów. Obecne podejście polega na przekwalifikowaniu modelu z dodatkowymi danymi wejściowymi i inną architekturą, co wymaga czasu i zasobów obliczeniowych.\n",
    "\n",
    "Podczas tego praktycznego warsztatu omówimy metodę, która pozwala nam generować obiekty o danym zestawie atrybutów bez ponownego uczenia modelu bazowego. W tym celu wykorzystamy modele normalizing flows – Conditional Masked Autoregressive Flow i Conditional Real NVP oraz sieci Plugin, w wyniku których powstaje Flow Plugin Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cel warsztatów:\n",
    " * Praktyczne zapoznanie się z modelami Normalizing Flows oraz biblioteką nflows\n",
    " * Praktyczne zapoznanie się z metodą Flow Plugin Network\n",
    "\n",
    "Agenda:\n",
    " * Wstęp do modeli generatywnych\n",
    " * Praktyczny wstęp do Normalizing Flows z wykorzystaniem pakietu **nflows**.\n",
    " * Metoda Flow Plugin Network (FPN)\n",
    " * Wykorzystanie metody FPN do warunkowego generowania obrazów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Wstęp do modeli generatywnych\n",
    "\n",
    "## Modele dyskryminatywne vs. modele generatywne\n",
    "\n",
    "**Model dyskryminatywny** modeluje warunkowego prawdopodobieństwo ${P(Y\\mid X=x)}$ zmiennej celu Y, biorąc pod uwagę obserwację x.\n",
    "\n",
    "*Przykłady*: Regresja Logistyczna, Drzewa decyzyjne.\n",
    "\n",
    "**Model generatywny** modelu rozkład łączny prawdopodobieństwa ${P(X, Y)}$ na danej zmiennej obserwowalnej X i zmiennej docelowej Y.\n",
    "\n",
    "*Przykłady*:\n",
    "\n",
    "![](figures/three-generative-models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalizing Flows\n",
    "\n",
    " Normalizing Flows reprezentują grupę modeli generatywnych, które można efektywnie trenować poprzez bezpośrednią estymację wiarogodności dzięki zastosowaniu wzoru na zmianę zmiennej. W praktyce wykorzystują one szereg (parametrycznych) funkcji odwracalnych: $\\mathbf{y}=\\mathbf{f}_n \\circ \\dots \\circ \\mathbf{f}_1(\\mathbf{z})$. Zakładając, że dany rozkład bazowy $p(\\mathbf{z})$ dla $\\mathbf{z}$, log likelihood dla $\\mathbf{y}$ jest podane przez $\\log p(\\mathbf{y}) = \\ log p(\\mathbf{z}) - \\sum_{n=1}^N \\log \\left| \\det \\frac{\\partial \\mathbf{f}_n}{\\partial \\mathbf{z}_{n-1}} \\right|$. W praktycznych zastosowaniach $p(\\mathbf{y})$ reprezentuje rozkład obserwowalnych danych, a $p(\\mathbf{z})$ jest zwykle zakładany jako rozkład normalny z niezależnymi komponentami.\n",
    "\n",
    "\n",
    "\n",
    "![](figures/normalizing-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Praktyczny wstęp do Normalizing Flows z wykorzystaniem pakietu **nflows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Omówienie modeli:\n",
    "  * NICE\n",
    "  * RealNVP\n",
    "  * MAF\n",
    "\n",
    "Omówienie warunkowych modeli:\n",
    "  * Conditional NICE\n",
    "  * Conditional RealNVP\n",
    "  * Conditional MAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Konstrukcja biblioteki nflows\n",
    "\n",
    "Krótkie omówienie biblioteki\n",
    "\n",
    "  * Distributions - Bazowe rozkłady prawdopodobieństwa\n",
    "  * Flows - Przykładowe implementacje modeli normalizing flows\n",
    "  * Nn - Implementacje sieci neuronowych budujących bloki modelu\n",
    "  * Transforms - Implementacje bloków transformujących w normalazing flows\n",
    "  * Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Toy example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_moons(128, noise=.1)\n",
    "plt.scatter(x[:, 0], x[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NICE (Non-linear Independent Component Estimation)\n",
    "\n",
    "Model NICE (Non-linear Independent Component Estimation) implementuje model normalizing flows poprzez układanie w stos sekwencji odwracalnych bijektywnych funkcji transformacji. W każdej bijekcji, znanej jako additive coupling layer, wymiary wejściowe są podzielone na dwie części:\n",
    "\n",
    " - Pierwsze wymiary pozostają takie same;\n",
    " - Druga część, do wymiarów, podlega transformacji addytywnej, tj. dodawany jest komponent przesunięcia.\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\mathbf{u}_{1:d} &= \\mathbf{x}_{1:d} \\\\\n",
    "    \\mathbf{u}_{d+1:D} &= \\mathbf{x}_{d+1:D} + \\mu(\\mathbf{x}_{1:d})\n",
    "    \\end{cases}\n",
    "    \\Leftrightarrow\n",
    "    \\begin{cases}\n",
    "    \\mathbf{x}_{1:d} &= \\mathbf{u}_{1:d} \\\\\n",
    "    \\mathbf{x}_{d+1:D} &= \\mathbf{u}_{d+1:D} - \\mu(\\mathbf{u}_{1:d})\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.nn import nets as nets\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.coupling import AdditiveCouplingTransform\n",
    "from nflows.transforms.normalization import BatchNorm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class NICE(Flow):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features,\n",
    "            hidden_features,\n",
    "            num_layers=2,\n",
    "            num_blocks_per_layer=2,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=0.0,\n",
    "            batch_norm_within_layers=False,\n",
    "            batch_norm_between_layers=False,\n",
    "    ):\n",
    "\n",
    "        mask = torch.ones(features)\n",
    "        mask[::2] = -1\n",
    "\n",
    "        def create_resnet(in_features, out_features):\n",
    "            return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                num_blocks=num_blocks_per_layer,\n",
    "                activation=activation,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=batch_norm_within_layers,\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            transform = AdditiveCouplingTransform(mask=mask, transform_net_create_fn=create_resnet)\n",
    "            layers.append(transform)\n",
    "            mask *= -1\n",
    "            if batch_norm_between_layers:\n",
    "                layers.append(BatchNorm(features=features))\n",
    "\n",
    "        super().__init__(\n",
    "            transform=CompositeTransform(layers),\n",
    "            distribution=StandardNormal([features]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nice = NICE(features=2, hidden_features=4, num_layers=1, num_blocks_per_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdźmy komponenty modelu\n",
    "nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nice = NICE(features=2, hidden_features=4, num_layers=4, num_blocks_per_layer=2)\n",
    "nice_opt = optim.Adam(nice.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, opt, num_iter=5000, iter_log=5000):\n",
    "    for i in range(num_iter):\n",
    "        x, y = datasets.make_moons(128, noise=.1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        opt.zero_grad()\n",
    "        loss = -model.log_prob(inputs=x).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i + 1) % iter_log == 0:\n",
    "            xline = torch.linspace(-1.5, 2.5, 100)\n",
    "            yline = torch.linspace(-.75, 1.25, 100)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zgrid = model.log_prob(xyinput).exp().reshape(100, 100)\n",
    "\n",
    "            plt.contourf(xgrid.numpy(), ygrid.numpy(), zgrid.numpy())\n",
    "            plt.title('iteration {}'.format(i + 1))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(nice, nice_opt, num_iter=10000, iter_log=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Modyfikując hiperparametry modelu wytrenuj model, który lepiej odwzoruje prawdziwy rozkład danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Badanie podstawowych funkcjonalności modeli\n",
    "#### Samplowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_moons(128, noise=.1)\n",
    "x = torch.tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = nice.sample(1000).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].scatter(x[:, 0], x[:, 1])\n",
    "ax[0].set_title('Data')\n",
    "\n",
    "ax[1].scatter(samples[:, 0], samples[:, 1])\n",
    "ax[1].set_title('Samples from NICE')\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RealNVP\n",
    "\n",
    "Model RealNVP (Real-valued Non-Volume Preserving) jest rozszerzeniem modelu NICE, które opiera się na warstwie znanej jako Affine Coupling Layer. Analogicznie do modelu NICE, wymiary wejściowe są podzielone na dwie części:\n",
    "\n",
    " - Pierwsze wymiary pozostają takie same;\n",
    " - Druga część, do wymiarów, podlega transformacji afinicznej („scale-and-shift”) i zarówno parametry skali, jak i przesunięcia są funkcjami pierwszych wymiarów.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\mathbf{u}_{1:d} &= \\mathbf{x}_{1:d} \\\\\n",
    "    \\mathbf{u}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp{(\\sigma{(\\mathbf{x}_{1:d})})} + \\mu(\\mathbf{x}_{1:d})\n",
    "    \\end{cases}\n",
    "    \\Leftrightarrow\n",
    "    \\begin{cases}\n",
    "    \\mathbf{x}_{1:d} &= \\mathbf{u}_{1:d} \\\\\n",
    "    \\mathbf{x}_{d+1:D} &= (\\mathbf{u}_{d+1:D} - \\mu(\\mathbf{u}_{1:d})) \\odot \\exp{(-\\sigma{(\\mathbf{u}_{1:d})})}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Na podstawie powyższego kodu modelu NICE oraz biblioteki nflows zaimplementuj model RealNVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# HINT: from nflows.transforms import ...\n",
    "\n",
    "\n",
    "class RealNVP(Flow):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features,\n",
    "            hidden_features,\n",
    "            num_layers=2,\n",
    "            num_blocks_per_layer=2,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=0.0,\n",
    "            batch_norm_within_layers=False,\n",
    "            batch_norm_between_layers=False,\n",
    "    ):\n",
    "\n",
    "        mask = torch.ones(features)\n",
    "        mask[::2] = -1\n",
    "\n",
    "        def create_resnet(in_features, out_features):\n",
    "            return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                num_blocks=num_blocks_per_layer,\n",
    "                activation=activation,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=batch_norm_within_layers,\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            transform = AdditiveCouplingTransform(mask=mask, transform_net_create_fn=create_resnet)\n",
    "            layers.append(transform)\n",
    "            mask *= -1\n",
    "            if batch_norm_between_layers:\n",
    "                layers.append(BatchNorm(features=features))\n",
    "\n",
    "        super().__init__(\n",
    "            transform=CompositeTransform(layers),\n",
    "            distribution=StandardNormal([features]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_nvp = RealNVP(features=2, hidden_features=4, num_layers=2, num_blocks_per_layer=2)\n",
    "real_nvp_opt = optim.Adam(real_nvp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(real_nvp, real_nvp_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dalsze badanie podstawowych funkcjonalności modeli\n",
    "#### Transformacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior_samples = real_nvp._distribution.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior_samples_numpy = prior_samples.detach().numpy()\n",
    "\n",
    "plt.scatter(prior_samples_numpy[:, 0], prior_samples_numpy[:, 1])\n",
    "plt.title('Próbki z rozkładu bazowego')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nf_blocks = real_nvp._transform._transforms\n",
    "nf_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for transform in nf_blocks:\n",
    "    print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "consequitive_samples = [prior_samples]\n",
    "current_sample = prior_samples\n",
    "\n",
    "for transform in nf_blocks:\n",
    "    current_sample, _ = transform(current_sample)\n",
    "    consequitive_samples.append(current_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for s in consequitive_samples:\n",
    "    s = s.detach().numpy()\n",
    "    plt.scatter(s[:, 0], s[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Masked Autoregressive Flow (MAF)\n",
    "\n",
    "Masked Autoregressive Flow to przykład modelu Normalizing Flows, który wykorzystuje połączenie z modelami autoregresyjnymi. W praktyce oznacza to, że używa reguły łańcucha do rozłożenia dowolnej gęstości prawdopodobieństwa $p(\\mathbf{x})$ na iloczyn jednowymiarowych rozkładów warunkowych jako:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\Pi^{N}_{i=1} p(x_i|\\mathbf{x}_{1:i-1}) \\textit{.}$$\n",
    "\n",
    "W przypadku modelu MAF $p(x_i|\\mathbf{x}_{1:i-1})$ jest modelowane jako:\n",
    "$$p(x_i|\\mathbf{x}_{1:i-1}) = \\mathcal{N}(x_i|\\mu_i, (\\exp{(\\alpha_i)})^2 \\textit{,}$$\n",
    "gdzie $\\mu_i = f_{\\mu_i}(\\mathbf{x_{1:i-1}})$ i $\\alpha_i = f_{\\alpha_i}(\\mathbf{x_{1:i-1}})$\n",
    "\n",
    "\n",
    "*Przykład*: $p(x_1, x_2) = p(x_1)p(x_2|x_1)$, gdzie $p(x_1) = \\mathcal{N}(x_1|0, 4)$ i $p(x_2|x1) = \\mathcal{N}(x_2|x_1^2, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.permutations import RandomPermutation, ReversePermutation\n",
    "\n",
    "\n",
    "class MaskedAutoregressiveFlow(Flow):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features,\n",
    "            hidden_features,\n",
    "            num_layers=2,\n",
    "            num_blocks_per_layer=2,\n",
    "            use_residual_blocks=True,\n",
    "            use_random_masks=False,\n",
    "            use_random_permutations=False,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=0.0,\n",
    "            batch_norm_within_layers=False,\n",
    "            batch_norm_between_layers=False,\n",
    "    ):\n",
    "\n",
    "        if use_random_permutations:\n",
    "            permutation_constructor = RandomPermutation\n",
    "        else:\n",
    "            permutation_constructor = ReversePermutation\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(permutation_constructor(features))\n",
    "            layers.append(\n",
    "                MaskedAffineAutoregressiveTransform(\n",
    "                    features=features,\n",
    "                    hidden_features=hidden_features,\n",
    "                    num_blocks=num_blocks_per_layer,\n",
    "                    use_residual_blocks=use_residual_blocks,\n",
    "                    random_mask=use_random_masks,\n",
    "                    activation=activation,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    use_batch_norm=batch_norm_within_layers,\n",
    "                )\n",
    "            )\n",
    "            if batch_norm_between_layers:\n",
    "                layers.append(BatchNorm(features))\n",
    "\n",
    "        super().__init__(\n",
    "            transform=CompositeTransform(layers),\n",
    "            distribution=StandardNormal([features]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "maf = MaskedAutoregressiveFlow(features=2, hidden_features=16, num_layers=4)\n",
    "maf_opt = optim.Adam(maf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(maf, maf_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Zweryfikuj jak wyglądają kolejne transformacje modelu MAF analogicznie do przykładu RealNVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Przerwa (10 min.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional NICE\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\mathbf{u}_{1:d} &= \\mathbf{x}_{1:d} \\\\\n",
    "    \\mathbf{u}_{d+1:D} &= \\mathbf{x}_{d+1:D} + \\mu(\\mathbf{x}_{1:d}, \\mathbf{c})\n",
    "    \\end{cases}\n",
    "    \\Leftrightarrow\n",
    "    \\begin{cases}\n",
    "    \\mathbf{x}_{1:d} &= \\mathbf{u}_{1:d} \\\\\n",
    "    \\mathbf{x}_{d+1:D} &= \\mathbf{u}_{d+1:D} - \\mu(\\mathbf{u}_{1:d}, \\mathbf{c})\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class cNICE(Flow):\n",
    "    def __init__(\n",
    "            self,\n",
    "            features,\n",
    "            hidden_features,\n",
    "            context_features,  # New component\n",
    "            num_layers=2,\n",
    "            num_blocks_per_layer=2,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=0.0,\n",
    "            batch_norm_within_layers=False,\n",
    "            batch_norm_between_layers=False,\n",
    "    ):\n",
    "\n",
    "        mask = torch.ones(features)\n",
    "        mask[::2] = -1\n",
    "\n",
    "        def create_resnet(in_features, out_features):\n",
    "            return nets.ResidualNet(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                hidden_features=hidden_features,\n",
    "                context_features=context_features,  # New component\n",
    "                num_blocks=num_blocks_per_layer,\n",
    "                activation=activation,\n",
    "                dropout_probability=dropout_probability,\n",
    "                use_batch_norm=batch_norm_within_layers,\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            transform = AdditiveCouplingTransform(mask=mask, transform_net_create_fn=create_resnet)\n",
    "            layers.append(transform)\n",
    "            mask *= -1\n",
    "            if batch_norm_between_layers:\n",
    "                layers.append(BatchNorm(features=features))\n",
    "\n",
    "        super().__init__(\n",
    "            transform=CompositeTransform(layers),\n",
    "            distribution=StandardNormal([features]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_nice = cNICE(features=2, hidden_features=8, num_layers=4, context_features=1)\n",
    "c_nice_opt = optim.Adam(c_nice.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_conditional(model, opt, num_iter=5000, iter_log=5000):\n",
    "    for i in range(num_iter):\n",
    "        x, y = datasets.make_moons(128, noise=.1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)  # New\n",
    "        opt.zero_grad()\n",
    "        loss = -model.log_prob(inputs=x, context=y).mean()  # New\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i + 1) % iter_log == 0:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "            xline = torch.linspace(-1.5, 2.5, 100)\n",
    "            yline = torch.linspace(-.75, 1.25, 100)\n",
    "            xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "            xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zgrid0 = model.log_prob(xyinput, torch.zeros(10000, 1)).exp().reshape(100, 100)\n",
    "                zgrid1 = model.log_prob(xyinput, torch.ones(10000, 1)).exp().reshape(100, 100)\n",
    "\n",
    "            ax[0].contourf(xgrid.numpy(), ygrid.numpy(), zgrid0.numpy())\n",
    "            ax[1].contourf(xgrid.numpy(), ygrid.numpy(), zgrid1.numpy())\n",
    "            fig.suptitle('iteration {}'.format(i + 1))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_conditional(c_nice, c_nice_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Wygeneruj próbki z modelu Conditional NICE dla górnego oraz dolnego półksiężyca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (10 min.)\n",
    "\n",
    "Stwórz model Conditional RealNVP oraz model Conditional MAF analogicznie do poprzedniego przykładu. Wytrenuj przykładowe modele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional RealNVP\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\mathbf{u}_{1:d} &= \\mathbf{x}_{1:d} \\\\\n",
    "    \\mathbf{u}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp{(\\sigma{(\\mathbf{x}_{1:d}, \\mathbf{c})})} + \\mu(\\mathbf{x}_{1:d}, \\mathbf{c})\n",
    "    \\end{cases}\n",
    "    \\Leftrightarrow\n",
    "    \\begin{cases}\n",
    "    \\mathbf{x}_{1:d} &= \\mathbf{u}_{1:d} \\\\\n",
    "    \\mathbf{x}_{d+1:D} &= (\\mathbf{u}_{d+1:D} - \\mu(\\mathbf{u}_{1:d}, \\mathbf{c})) \\odot \\exp{(-\\sigma{(\\mathbf{u}_{1:d}, \\mathbf{c}))}}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional Masked Autoregressive Flow (MAF)\n",
    "\n",
    "$$p(\\mathbf{x} | \\mathbf{c}) = \\Pi^{N}_{i=1} p(x_i|\\mathbf{x}_{1:i-1}, \\mathbf{c}) \\textit{,}$$\n",
    "gdzie\n",
    "$$p(x_i|\\mathbf{x}_{1:i-1}, \\mathbf{c}) = \\mathcal{N}(x_i|\\mu_i, (\\exp{(\\alpha_i)})^2 \\textit{,}$$\n",
    "gdzie $\\mu_i = f_{\\mu_i}(\\mathbf{x_{1:i-1}}, \\mathbf{c})$ i $\\alpha_i = f_{\\alpha_i}(\\mathbf{x_{1:i-1}}, \\mathbf{c})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metoda Flow Plugin Network (FPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Koncepcja Plugin Network\n",
    "\n",
    "![Plugin Network](figures/plugin_koperski.png)\n",
    "\n",
    "Pomysł:\n",
    " - Rozszerzenie istniejącej sieci neuronowej bez dodatkowego treningu, np. gdy pojawią się dane z dodatkową informację (eng. partial evidence)\n",
    "\n",
    "Założenia koncepcji:\n",
    " - Nie przetrenowujemy oryginalnej sieci lecz jedynie trenujmy komponent plugin\n",
    " - Czas predykcji modelu jest jedynie nieznacznie większy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Koncepcja Flow Plugin Network\n",
    "\n",
    " - Rozszerzenie Plugin Networku do modeli generatywnych\n",
    " - Wykorzystuje jako model bazowy - VAE\n",
    " - Wykorzystuje Conditional Normalizing Flows\n",
    " - Nie przetrenowujemy modelu bazowego\n",
    "\n",
    "\n",
    "![](figures/schema-general.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trening modelu\n",
    "\n",
    "### Kodowanie zbioru treningowego do reprezentacji ukrytej\n",
    "![](figures/schema-training-encode.png)\n",
    "\n",
    "### Trenowanie modelu Normalizing Flows\n",
    "![](figures/schema-training-train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generowanie próbek\n",
    "\n",
    "![](figures/schema-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Manipulowanie cechami obiektu\n",
    "\n",
    "### Kodowanie obiektu\n",
    "![](figures/schema-image-manipulation-encoding.png)\n",
    "\n",
    "### Dekodowanie obiektu\n",
    "![](figures/schema-image-manipulation-decode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Wykorzystanie metody FPN do warunkowego generowania obrazów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sample z modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://github.com/lyeoni/pytorch-mnist-VAE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, h_dim3, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        # encoder part\n",
    "        self.e_fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.e_fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.e_fc3 = nn.Linear(h_dim2, h_dim3)\n",
    "        self.fc_mu = nn.Linear(h_dim3, z_dim)\n",
    "        self.fc_logvar = nn.Linear(h_dim3, z_dim)\n",
    "        # decoder part\n",
    "        self.d_fc1 = nn.Linear(z_dim, h_dim3)\n",
    "        self.d_fc2 = nn.Linear(h_dim3, h_dim2)\n",
    "        self.d_fc3 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.d_fc4 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.e_fc1(x))\n",
    "        h = F.relu(self.e_fc2(h))\n",
    "        h = F.relu(self.e_fc3(h))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)  # mu, log_var\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)  # return z sample\n",
    "\n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.d_fc1(z))\n",
    "        h = F.relu(self.d_fc2(h))\n",
    "        h = F.relu(self.d_fc3(h))\n",
    "        return torch.sigmoid(self.d_fc4(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, self.x_dim))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, h_dim3=128, z_dim=40)\n",
    "model.load_state_dict('models/VAE.pkt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_vae(vae, z_dim, n_samples=64):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, z_dim).to(DEVICE)\n",
    "        return vae.decoder(z).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = sample_vae(model, z_dim=40, n_samples=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "save_image(samples.view(64, 1, 28, 28), f'results/samples_vae.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](results/samples_vae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Kodowanie danych do latent space'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_latent_space(model, loader):\n",
    "    zs, ys = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.view(-1, 28*28)\n",
    "        zs.append(model.sampling(*model.encoder(x)))\n",
    "        ys.append(y)\n",
    "    zs, ys = torch.cat(zs).detach().numpy(), torch.cat(ys).detach().numpy()\n",
    "\n",
    "    latent_space = pd.DataFrame(zs)\n",
    "    labels = pd.DataFrame({'y': ys})\n",
    "    return latent_space, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_latent_space, train_labels = get_latent_space(model, train_loader)\n",
    "test_latent_space, test_labels = get_latent_space(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_latent_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Zwizualizuj przestrzeń ukrytą z wykorzystaniem PCA, gdzie kolor będzie oznaczać klasę obiektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trenowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_ohe = pd.get_dummies(train_labels['y'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flow = cMaskedAutoregressiveFlow(features=40, hidden_features=40, context_features=10, num_layers=5, num_blocks_per_layer=3)\n",
    "opt = optim.Adam(flow.parameters())\n",
    "\n",
    "batch_size = 1000\n",
    "num_epochs = 10\n",
    "\n",
    "for i in range(num_epochs * (len(train_latent_space) // batch_size)):\n",
    "    x = train_latent_space.sample(batch_size)\n",
    "    y = train_labels_ohe.loc[x.index]\n",
    "\n",
    "    x = torch.tensor(x.values, dtype=torch.float32)\n",
    "    y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = -flow.log_prob(inputs=x, context=y).mean()\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sprawdzamy skuteczność modelu - samplowanie wybranej klasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 64\n",
    "\n",
    "samples_latent_space_2 = flow.sample(num_samples, context=torch.Tensor([[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]))\n",
    "samples_latent_space_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples_2 = model.decoder(samples_latent_space_2.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_image(samples_2.view(num_samples, 1, 28, 28), f'results/samples_flow_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](results/samples_flow_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ćwiczenie (5 min.)\n",
    "\n",
    "Wygeneruj próbki dla innych klas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inne metody\n",
    "\n",
    "## PluGeN\n",
    "\n",
    "### Intuicja\n",
    "\n",
    "![](figures/PluGeN_a.png)\n",
    "![](figures/PluGeN_b.png)\n",
    "\n",
    "### Metoda\n",
    "\n",
    "![](figures/PluGeN_schema.png)\n",
    "\n",
    "## StyleFlow\n",
    "\n",
    "### Możliwości\n",
    "\n",
    "![](figures/styleflow_teaser.png)\n",
    "\n",
    "### Metoda\n",
    "\n",
    "![](figures/styleflow_schema.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
